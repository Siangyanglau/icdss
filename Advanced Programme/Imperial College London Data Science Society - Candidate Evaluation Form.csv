Timestamp,What is your name?,What is your background (choose closest)?,If you selected other above then specify your background:,1. What is your favourite programming language? Why?,"2. What libraries and frameworks related to data science you know? If you know many, just list those most relevant to you or ones you like/use most.","3. Do you have any online portfolio to show your code (e.g. GitHub, BitBucket)? If it is not perfect we still would like to see it (it rarely is!). If you do not have one do not worry.",4. Write a code in any language and using any plotting libraries you know to create a plot as similar as possible to the one given above using data from the provided CSV file.,5. Do you know what DAU is? Why do you think it is a popular unit?,6. What do you think is most important in data visualisation? Do you have any data presentation examples that you appreciate or admire (share link or explain shortly). What do you like most about it?,7. If you throw 2 dices at the same time and compute their sum. What is the probability of sum being 8?,8. How many people should attend your party that probability of at least 2 people having birthday on the same day being more than half? Do you expect the number to be the same for real data?,9. Can you think of a method to estimate the value of pi using methods known from probability theory and statistics?,10. How would you categorise fruit in the above pictures into apples and oranges? What issues do you expect?,11. What is overfitting?,12. How can you prevent overfitting?,13. Provide SQL query (in any dialect) that counts the number of 2nd degree followers for each followee.,14. Do you know any other query languages than SQL that could make answering question about degree-of-n followers easier?,15. How can you be sure that the answer your query engine returns is correct? Think both example size and at actual Twitter scale.,16. Explain in few sentences what convolutional network is.,17. What ConvNets can be used for?,18. How ConvNets can be trained? What is pretraining and why is it important?,What do you want to get out of Data Science Society?,What would be the ideal amount of time in every week for you to spend on Data Science Society activities?
2016/01/31 10:05:34 pm GMT,Mengran li,"Other (please, specify)",Electrical and Electronic Engineering,C++; fast development,N/A,No,,,,May-36,"730, no",,Color. Orange is orange. ,,,,,,,,,Some trainging,3
2016/02/01 6:20:37 pm GMT,Conor Bradley,Physics,,"Python. The majority of my experience has been in this language; it's well suited to my purposes which are primarily scientific modelling or data-plotting purposes. Python syntax is also very intuitive compared to many other languages, and efficient (in terms of writing) when compared to, for example, Java. Using interpreted code is generally quicker to debug than compiled code, and the vast range of libraries exceeds most other languages I've come across.","NumPy, SciPy, Matplotlib are all libraries which I have used extensively - more or less in every piece of code I've written. I've also briefly used pandas (large array handling).","I do not have an online portfolio. The bulk of my computing experience has arisen from university work - I'm not comfortable with putting this online for plagiarism reasons, but can show in person if required.","#Python""""""DAU Stacked BarchartConor Bradley 01.02.2016""""""import numpy as npimport matplotlib.pyplot as pltimport csvwith open('weekly_data.csv', 'rb') as csvfile:    data = csv.reader(csvfile)    store = [row for row in data]    day = [store[i][0] for i in np.arange(1,6,1)]    a = [int(store[i][1]) for i in np.arange(1,6,1)]    b = [int(store[i][2]) for i in np.arange(1,6,1)]    c = [int(store[i][3]) for i in np.arange(1,6,1)]    d = [int(store[i][4]) for i in np.arange(1,6,1)]    e = [int(store[i][5]) for i in np.arange(1,6,1)]ind = np.arange(5) + 0.5width = 0.5fig, ax = plt.subplots()ax.grid(True)p0 = plt.bar(ind, a,  width, alpha=0.7, color = 'b')p1 = plt.bar(ind, b, width, bottom = a, alpha=0.7, color = 'c')p2 = plt.bar(ind, c, width, bottom = [a[i]+b[i] for i in xrange(len(a))], alpha=0.7, color = '0.75')p3 = plt.bar(ind, d, width, bottom = [a[i]+b[i]+c[i] for i in xrange(len(a))], alpha=0.7, color = '#ffb347')p4 = plt.bar(ind, e, width, bottom = [a[i]+b[i]+c[i]+d[i] for i in xrange(len(a))], alpha=0.7, color = 'r')plt.title('user activity per service')plt.ylabel('DAU [10e3]')plt.xlabel('Week Day')plt.xticks (ind + width/2.,day)plt.legend( (p0[0], p1[0], p2[0], p3[0], p4[0]), ('A', 'B', 'C', 'D', 'E') )plt.tight_layout()plt.show()    ","Daily Active Users. Useful gauge for measuring general popularity in the short term. If a recurring pattern, low popularity day (e.g. friday) could be identified as a good day to push updates.","When looking at data, the most important aspect to me if that the 'point' of the visualisation is apparent, rather than the cluttered infographics etc. which are often produced. It is also imperative that the data is put in context, as many productions use cut axes which paint an inaccurate picture. I was recently impressed by: http://graphics.wsj.com/infectious-diseases-and-vaccines/ , a heat map of infectious diseases and their decline with vaccines. The sheer simplicity for such a large amount of medical data, and the clear trend, is appealing. ",May-36,"23. The number is probably less in the real world. The birth rate is not uniform - certain months (eg late summer) have higher birth rates, and certain birth dates for a given age range (say children at a birthday party) will be more probable as more births occur during the week when hospitals are better staffed. Thus p(match) is higher.","Pi can be estimated by Monte Carlo methods. A unity radius circle is placed in a square of side length 2. This square is discretised into n x n points of small spacing. By randomly selecting points on the grid, and testing the ratio of points found to be inside and outside of the circle, a value for pi is found (= 4*ratio).","Apples are yellow-green, Oranges are...orange. The oranges are of similar size, but there appears to be two types of apple, one larger, one smaller. There are also red patches on the apples which might overlap with the oranges. There is a lemon which falls under neither category, but matches the smaller apples. Furthermore, the background colours may cause issue with any attempt to solve this by machine vision. Easiest test would be oranges and ""not-oranges"".","The process by which a numerical fit matches statistical error (or noise), rather than the underlying relationship governing some behaviour. ","Ensure that the number of parameters used in a model is not overly excessive - if this is true, there is likely some (unphysical) fit which will better minimise the fitting condition (eg regression fit). The extreme example of this is the Lagrange polynomial going through all n points.",Have never used SQL. ,"Again, no experience.","Of course, number of followers returned must be less than total user count. It must also be <= the sum of the followers of each primary follower (i.e. the maximum 2ndary follow count occurs if none of the followers of primary followers directly follow the user). This provides an absolute limit. ","A convolutional network is a one direction (feed forward) neural network, used to deal with an input image (the visual field). Input nodes -> output nodes in the most general sense. Based on basic biological systems, they are written such that each neuron covers a small area of the image, which a slight overlap between neurons. Small clusters of neurons produce outputs which are pooled by various layers.  Little pre-processing is used, convolution operations are used to minimise free parameters.",Used  primarily for machine vision (image recognition). Some medical applications e.g. drug discovery. Google's Deepmind recently became the first ConvNet tech to beat a professional GO player.,"Important to ensure that the model does not overfit. This leads to completely invalid results. Typically, a ConvNet will be trained on a very large data set, this enables a set of ""ground rules"" to be identified which will govern the behaviour of the technology. When such datasets are not available, they are found from alternative sources, such as related fields. Finetuning then occurs based on pretraining on a smaller, more specific dataset.","In simple terms, I would be keen to develop my current knowledge beyond Physics computational models and analysis. I have had no opportunity to work with machine learning processes during my degree, and would find that prospect incredibly exciting. More broadly, it is clear that the use of Data science is becoming more and more ubiquitous in academia, finance, security - and of course social networks. The value of this work is growing rapidly. In this regard, and with Imperial's growing stature in the field, I believe that there is a strong opportunity to become a part of a really exciting venture. This chance to work with leading industry experts and academics is rare.",3
2016/02/02 12:33:49 am GMT,Chu Yong Mark Lee,Computer Science,,"Python. Wide array of data science tools, easy to understand","scikit-learn, gensimProbably going to use pandas more in the future",https://github.com/astraldawn/newschain_archive - A detailed report can be found in the archive,"import pandas as pdimport matplotlibimport matplotlib.pyplot as pltmatplotlib.style.use('ggplot')data = pd.read_csv('weekly_data.csv', index_col=0)display = data.plot(			kind='bar', 			title=""User activity per service"", 			stacked=True, 			grid=True, 			colormap=""plasma""		)display.set_xlabel(""Week day"")display.set_ylabel(""DAU [10e3]"")plt.show()",Daily average users. It is an accurate measure of consistent usage of a product.,It should enhance our understanding of the issue at hand AND / OR provide an excellent summary. Nice video - https://www.youtube.com/watch?v=hVimVzgtD6w,13.90%,"23. No, the assumption made for the previous part is that birthdays follow a uniform distribution, doesn't hold for real data",Could not think of it offhand,"Haar cascades. Not exactly familiar but given that apples and oranges look quite similar, could get apples being classified as oranges and vice-versa.",When a trained classifier fits patterns in the data that have occurred due to chance.,Introduce a cross validation set and check that the trained classifier performs equally well on both the training and cross validation set. Regularisation may be performed as well.,Not familiar with traversing a graph in SQL,No,Write code in some other language to traverse the graph and manually verify the return is correct. Not sure how Twitter would handle at scale but any solution involving joins would probably run too slowly to be useful.,Convolution neural network. Have heard of it but never used it.,Deep learning problems,Don't know,"Learn more about aspects of data science that I am unfamiliar with, especially with regards to scale and deep learning",3
2016/02/02 12:41:51 am GMT,Kin Tat Yiu (Kenneth),Physics,,"C++, first language I ever used. Now, I start to use more python.",NumPy,no,"matplotlib, var.unstack().plot(kind='bar',stacked=True,  color=['red','blue','pink',etc...], grid=False)","Daily average users, It shows how many active user are using your application",It needs to be clear to reader without explanation. I really like the Data Visualization Field on Twitter. They put different keywords in different circle with different size and color. It is a direct method to make the reader to understand it at the first glance.,01-Jun,"23, no, because it assumes the birthday of the people are uniformly distributed while in most of the cases, it is not. This is because not all the days are equally likely.",Estimating Pi using Monte Carlo Simulation,"Use size, use color. It is obvious that there are apple, orange and lemon. However, for machine, they may not be able to distinguish the similar colors between apple and lemon. Therefore, more parameters should be put in the training set such as shape or color gradient.",There are too many parameters for the model to fit.,Use less parameters by removing the parameters that would not significantly increase your error rate. ,"COUNT DISTINCT follower FROM followee.follower where  followee.follower != followee EXCEPTSELECT follower FROM follwee.follower where followee = 1(too lazy to google the left join or inner join, probably all wrong)",NoSQL,"6 degrees of separation, if the whole data size is not connected within 6 degrees of separation, then you probably are getting the wrong result.","People use local receptive fields, shared weights, and pooling to do deep machine learning. It basically uses multilayer perceptron to distinguish patterns.","It uses the spatial structure and local connectivity to analyze the pattern and then adapt. Hence, it is good at classifying images and pattern recognition. It can also apply to speech recognition.","Most of them just train it by unsupervised learning which is layer-wise pre-training. It is just the basic setup for the network so they can ""build up"" different layers and then apply it. It is important because we want to make sure they get a correct initialization, otherwise, even the network is well-trained afterwards, it will give a very different result due to the setting.",Fun,3
2016/02/03 2:33:43 pm GMT,Kane Wu,Computer Science,I also have a CPA and a law degree,"Java, Python. Java because it's so general. Python because it's so concise and so readable. ","Pandas, Numpy, Tableau, AMPL, Hadoop, Spark. I am also learning scikit-learn and tensorflow",,"import numpy as npimport matplotlib.pyplot as pltA = ([3,10,21,2,4])B = ([8,4,3,1,2])C = ([14,17,1,20,4])D = ([13,3,3,12,4])E = ([1,10,2,14,6])place = np.arange(5)width = 0.2p1 = plt.bar(place, A, width, color='#0000ff')p2 = plt.bar(place, B, width, color='#6699ff',bottom=A)p3 = plt.bar(place, C, width, color='#ccccff',bottom=np.asarray(A)+np.asarray(B))p4 = plt.bar(place, D, width, color='#ffcc99',bottom=np.asarray(A)+np.asarray(B)+np.asarray(C))p5 = plt.bar(place, E, width, color='#ff3300',bottom=np.asarray(A)+np.asarray(B)+np.asarray(C)+np.asarray(D))plt.ylabel('DAU[10e3]')plt.xlabel('Weekday')plt.xticks(place + width/2., ('M', 'T', 'W', 'Th', 'F'))plt.legend((p1[0], p2[0], p3[0], p4[0], p5[0]), ('A', 'B', 'C', 'D', 'E'))plt.show()",Daily Active User. It is popular for websites and games. It's popular because it allows a website or game to know how many people are actually using their site and game each day. This is very important as users can translate to revenue.,Most important thing in data visualization is to have graphs that communicate data to your target audience. It should not just be about impressing people that you know tableau but about using data to tell stories. http://www.oecdbetterlifeindex.org/ I like the OCED index because it's pretty and it allows you to explore the data easily. At the same time it communicates important information in which it wants to communicate,5/36 = 13.89,23 people. No i do not expect the number to be the same as there is variance with the data so it can be either more or less. ,Buffon‰Ûªs needle,"Pattern recognition and machine learning. Oranges have certain characteristics such as it being orange and more round while the apples will have a stem and will be more red. If we use pattern recognition and machine learning, there is not enough data with that one picture to feed it into the computer for it to learn. ",Not separating data into test and training data causing the criteria for training the data to be the same as the testing data.,cross - validation.,"select A1.followee, count(distinct A2.follower) from Following A1    inner join    Following A2    on A1.follower = A2.followee    where not exists    (      select followee,follower from Following F      where exists      (        SELECT F1.followee,F2.follower from Following F1               inner join                Following f2                on f1.follower=f2.followee                where F.followee = F1.followee and F.follower = F2.follower      ) and A1.followee = F.followee and A2.follower = F.follower    )","Cypher, PQL, basically query languages for graphs","For the example size you can just look at the correct results and compare. For twitter scale, you can run for a smaller size like 50 records and see if the results are the same. Then scale up a little and check again if the results are the same to ensure that the returns are correct. ","It mimics a kind of neural network that has identical copies of the same neuron. The network will then have a lot of neurons while keeping the parameters small. Once a neuron has learn, it can be used in many places so that it can reduce error and make it easier to learn. The neurons will then fit into larger neurons until it fits into a fully connected layer. ","Pattern Recognition, Image Classification, Audio Recoginition","For an image classification problem, you train ConvNets by running the classification algorithim repeatedly. Then you also run backpropagation as well which basically is to check how the network classify things incorrectly and to fix it. Pretraining is unsupervised learning. It is important since it makes optimisation easier and it reduces overfitting. There are research that states that pretraining is not as important for large datasets. Since with large datasets you can basically reduce the impact of overfitting. ",Learn more about Machine Learning and Data Science as well as to compete. ,5
2016/02/05 12:30:43 am GMT,LU QI,"Other (please, specify)",Business Analytics,"Python, the syntax is straightforward, and its the first programming language I know",,,"If I have enough time to complete this graph, I'd choose using R and package ggplot.",,I think the most important thing in data visualisation is the message is clearly and successfully delivered to the audience,May-36,20,Buffon's needle,"Categorise them by size, colour and angle. Apples are generally larger than oranges, and in terms of colour, oranges are very easily distinguished, and to differentiate lemons and apples, in addition to size, lemons have small tuber with acute angle.",The model is too complex to fit the data too well. It may happen when we have too many parameters relative to the numbers of data.,Set a set of data for testing; use more data.,"SELECT ff.followee, COUNT(DISTINCT ff.follower)FROM(SELECT f1.followee, f2.followerFROM f1,f1 f2WHERE f1.follower=f2.followeeEXCEPTSELECT * from f1) ffGROUP BY ff.followee;where f1 is the table in the question.",,"Do testing on a small amount of data first, then run on the actual dataset.",,,,"training support on data science learning in every aspect, and that learning experience.",2
2016/02/06 4:52:37 pm GMT,Ambuj Agrawal,Business,,"C++ - Allows low level programming, working with compiler tools like LLVM and is object oriented","Theano, deepnet, SINGA and Mocha are the ones I like the most",https://github.com/ambujone,"Would do it in R to create the stacked bar plot, something likecounts <- table(mtcars$vs, mtcars$gear)barplot(counts, main=""user activity per service"",  xlab=""Week day"", ylab=""DAU[10e3]"",col=c(""darkblue"",""lightblue"",""grey"",""orange"",""red""), 	legend = rownames(counts))",DAU is the Data Acquisition Unit. Data Acquisition is the process of sampling signals that measure real world physical conditions.,"The most important thing according to me in data visualisation is the ease of making sense of the data.  The data presentation which I like are the ones used by statista (http://www.statista.com/) because the images are easy to understand, contains less text and allows amazing data visualisation.",5/36 or 14%,More than 23 people should attend my party to have the probability of at least 2 people having birthday on the same day being more than half. I would expect some variation with the real data but with a large enough sample it should be near 23 people for the probability of at least 2 people having birthday on the same day being more than half.,Monte Carlo method can be used to estimate the value of pi. More info: http://polymer.bu.edu/java/java/montepi/MontePi.html,The fruits in the above picture can be categorised using different machine learning algorithms. For example by using Fourier Feature Extraction on apples and oranges and then classifying them according to nearest neighbour classifier. Sometimes there might be oranges which are classified as apples due to the overlap of similarity between these fruits,Overfitting occurs when the model is too much biased on the current set of data. Then this model fails to generalise for new examples.,Overfitting can be prevented by making the model more generalised and using less number of features (which overfits with the given data),"select count(distinct t1.follower)     from t        join          t t1        on t.follower=t1.followee  where t.followee='1'// Can be done in a similar way for other numbers",Online analytical processing. More info: https://en.wikipedia.org/wiki/Online_analytical_processing,Random Testing,"Convolutional network is a neural network in which it is assumed that the inputs are images which allows us to encode certain properties in the network architecture. The three basic ideas used in convolutional network is local receptive fields, shared weights and pooling.","Video surveillance, visual object recognition, self navigating machines. Interesting paper: http://koray.kavukcuoglu.org/publis/lecun-iscas-10.pdf","ConvNets can be trained by using large number of labeled training samples. Pretraining is learning a nonlinear transformation of the output of the previous layer that captures the main variations in its input. Pretraining is required when the number of labeled training samples is small.",Improve my Data Science skills.,2
2016/02/07 6:55:07 am GMT,Eusebius Ngemera,"Other (please, specify)",Electrical & Electronic Engineering,Python because it's very versatile and is a quick way to turn ideas into code,MATLAB platform,https://github.com/eugenius1,"# Python 2.7# Eusebius Ngemera# ICDSS - plot barchart from CSV# 7 Feb 2016import numpy as npimport matplotlib.pyplot as pltdef just_numbers(numpy_void):    # remove the day from the numpy.void iteratable    return tuple(numpy_void)[1:]# Read data into numpy.ndarray with field names... week_data['A']weekly_data = np.genfromtxt('weekly_data.csv', dtype=None, delimiter=',', names=True)days = weekly_data['Day']N = len(days)x_pos = np.arange(N)width = 0.5colours = (""RoyalBlue"", ""LightSkyBlue"", ""Gainsboro"", ""DarkSalmon"", ""FireBrick"")columns = 'ABCDE'# Get accumulated positions for stacking bars# Aim is to have (accumulated) row for each additional columnsbtm = np.cumsum(map(just_numbers, weekly_data), axis=1) # accumulate columns one-by-onebtm = btm.T     # Transposep = []      # plot-handlesp.append( plt.bar(x_pos, weekly_data[columns[0]], width, align='center', color=colours[0]) )for i in range(1,N):    p.append( plt.bar(x_pos, weekly_data[columns[i]], width, align='center', color=colours[i%len(colours)], bottom=btm[i-1]))plt.xticks(x_pos, days)  # plot labels for each barplt.xlim([min(x_pos) - width, max(x_pos) + width])plt.xlabel('Week day')plt.ylabel('DUA [10e3]')plt.title('user activity per space')plt.legend(p, (char for char in columns))plt.grid(True)plt.show()","If you mean Daily Active Users, then sure it's a popular unit but not as much as Monthly active users, or even weekly. If you take the example of the gaming service there will be variations in daily aggregates but usage patterns will repeat weekly. DUA gives more information for example different services might be dominating on different days.","Rapid comprehension, almost instinctive. I like this example from Bloomberg's Terminal which shows a country's exports and imports to and from other countries. The scale of each trade with a country is clear from the thickness of the lines. Interaction is important too with visualisations. With this you can choose any of the countries shown to see its data.https://lippincottlibrary.files.wordpress.com/2013/11/china-deficit_reduced.jpg",5/36= 0.139,When probability of having unique birthdays becomes less than 1. It starts at 1 then lowers to ~364/365 then 364*363/365^2 and so on,PI digits are random so whatever you get is bound to be in there somewhere ,"Colour, roundness, texture (roughness),Depends on the camera, quality of the picture, lighting, other unknown objects in the picture",Overdoing,,I don't understand the question,Prolog,,,"Neural networks, break big problem into smaller (less computationally intensive) chunks",Repetition,Extend my knowledge and experience of Artificial Intelligence and programming,4
2016/02/07 5:48:51 pm GMT,Guillaume Paillot,Computer Science,,"Python:- Versatility- Lots of useful packages available- Easy to teach teammates","numpy, scipy",github.com/T0ll,"Matlab (would have done it on Python with matplotlib if I had time) :A=csvread('weekly_data.csv',1,1)B=fliplr(A)G=bar(B, 'stacked')title('user activity per service')xlabel('Week day')ylabel('DAU [10e3]')set(gca,'YTick', 0:10:50)set(gca,'XTickLabel',{'M', 'T', 'W', 'Th', 'F'})legend('A','B','C','D','E')G(1).FaceColor='b'G(2).FaceColor=[0.5,0.5,1]G(3).FaceColor=[0.8,0.8,0.8]G(4).FaceColor=[1,0.5,1]G(5).FaceColor='r'","Daily active user: Average number of users in a dayPopular unit to estimate to traffic to website. Usually preferred to the number of visits since it nullifies the impact of extremely frequent users.","A visualisation should always be self-supporting: the analyst do not need to explain anything as the visualisation should speak for itself. That is every element relevant to the understanding should be represented.One of my favorite data visualisation are dynamic sunburst diagram:http://bl.ocks.org/mbostock/5944371I like the fact that this visualisation has several layer of information. By clicking on the different layers, you can navigate from generic to specific.","(Are the dice balanced? :) )Total: 36 results possibles6 gives sum equal to 8 ([2,6] [3,5] [4,4] [4,4] [5,3] [6,2]) Probability=6/36=6","23 peopleWith real data, the number will be a bit lower since there are some dependencies between the probability of having one's birthday on a given day (Who said Valentine's day?)","A well-known method would rely on using Buffon's needle: On a sheet of paper, draw parallel lines with a fixed interval. Drop randomly some needles of another given length on the paper. Check if the needle crosses a line.We can calculate the theoritical probability ( which depends on Pi, the length of the needles, and the interval ) and by comparing it to the actual results give an estimation of pi  ","Step 1 : Define attributes to classify the fruitsex: color(using RGB as three distinct attirbutes, radius, roundness, texture)Step 2: Build a decision tree based on these attributes and the given sample (Could use the ID3 algorithm)Issues:- Overfitting: Risk of making the decision tree too specific- There appears to be a lemon for some reason ...","Overfitting is when, after 'improving' your classification, you have a smaller error on the training set but a higher error on the validation set. This usually means that your algorithm 'over-specifies' the classification criteria based on the training set","Pruning, k-fold cross-validation","SELECT a.followee, COUNT(DISTINCT b.follower)FROM following aJOIN following b ON a.follower=b.followeeGROUP BY a.followee;",MongoDB,I would test on smaller relevant subset. Twitter example: in one small country,"I have learned about neural networks, but know convolutional networks only by name. Convolutional networks are a kind of artificial neural network, which take into input different overlapping subset of a picture.",Image recognition,,"I want to be ready to work as a data scientist by the time I graduate. Pretty simple, don't you think :)",3
2016/02/08 11:29:50 am GMT,Maxim Khomiakov,Statistics,,"I enjoy working in R, I very much enjoy working in R. This is perhaps because of my main interest in programming is in data analysis, and as such, R is fantastic. It is able to minimise possible headache one might experience, working with linear algebra in Java,C++ or Python. It is simple, efficient, and great for data analytical applications.Using Numpy and scikit in Python would be my second choice.","In R the caret package is awesome. It gives you an array of ML methods, furthermore it allows for features selection, data splitting, graphical visualisation, and paralleled model fits.Recently I have started to play with the mxnet package, a very powerful deep learning package applicable with GPUs. ","Sure, please be noted this is part of my confidential work from my prior MSc thesis: https://github.com/terrencebrown/temporary_repository","################################## Import data & Initialisationlibrary(RColorBrewer)library(readr)X=read.csv(""weekly_data.csv"",header = T)brewcol=brewer.pal(5,""RdBu"")#Ensure correct factor orderingX$Day=ordered(X$Day,levels=c(""M"",""T"", ""W"",""Th"",""F"" )) #Ensure correct factor ordering#Plot the chartbarplot(as.matrix(t(X))[6:2,],names.arg = X$Day,col=brewcol[5:1],main=""user activity per service"",ylab=""DAU [10e3]"",border=T,xlab=""Week day"")#Add graphical parameterslegend(""topright"",colnames(X)[2:6],fill=brewcol[5:1])grid(5,6, lty = 3, lwd = 1,col=""black"")box(lty = 1,col=""black"")","Daily Average Users I would assume. The unit is highly relevant in order to monitor web traffic, and indeed when such traffic occurs (in this case days during the week). Website monitisation is highly dependent on the amount of users, actually using their services.","To get your point across, clearly. Visualisation of complex data should be rich in detail, but easy in comprehension. Therefore, even if the intended audience is technically mature, ambiguities should be minimised. This may be done by cleverly thinking of ways to use color, support lines, non-conventional figures and the like.I recently was quite fascinated by the real-time visualisation of cyber hack attacks, made by Norse Corporation: http://map.norsecorp.com/ - Very interesting. In particular when you start to add up the numbers, and realise how many presumed attacks are tried at U.S based servers.Furthermore, another visualisation that caught my attention, the Hatnote Listen to Wikipedia: http://listen.hatnote.com/ - This is wikipedia edits, user additions and corrections transformed into sound. I found myself enjoying the music, a sort of aesthetic randomness.","In order for the (fair) dice to have a sum of eight, there exists 5 possible outcomes for this to be true X= {[4,4],[3,5],[5,3],[6,2],[2,6]}. The total outcome space is 6*6=36 therefore the probability for this event to occur at any time during a throw is 5/36 = 13.8%","Simplifying for 365 days a year. We may assume the probability of people born on the same day by P(S) and people born on other days to be P(O). Then the total outcome space is P(S)+P(O) = 1, and as such we find that we are interested in modeling P(S) = 1 - P(O)The probability for one person to have a birthday on any of the 365 days a year is 1/(365/365), which means that if only one person were to attend the party, the probability for two people to have a birthday on the same day is 0. Similarly, the probability of any pair of four people to have a birthday on the same day, can be written as P(S) = 1 - [1/365/365 * 2/364/365 * 3/363/365 * 4/363/365 ] ~ 1, thus still infinitely small. This can also be written as P(S) = 1 - [365!/(365)^x] where x is the number of people required attend the party.Thus we want to compute P(S>=.5) = 1 - [365!/365^x*(365-x)!] = 23 people. The break happens at 22 (~47% probability) and then at 23 people the probability is 50.1%.For real data this would probably not be true, as one may have skewed or in another way non-normal samples.","Well, Perhaps if you acquire enough data on circles, or in any other way may sample a large enough portion of circle shapes. Then one could extract the mean diameter and mean circumreference of those circles, and look at the ratios. In principle, if the data was more or less representating true circles, Pi^ = E[C]/E[d] where E[C] is the expected value of all circle circumreferences, E[d] is the expected value of all circle diameters, and Pi^ would be estimated value of Pi.","The most simplest way, it seems, is to extract the oranges by binary thresholding. First one would identify the best colormap for amplifying the distinct orange intensities, then one could identify the mean intensity of those oranges, threshold the image and apply the binary mask to the image to identify the oranges. Likely noise may be reduced by using mixture models instead.The apples may prove slightly harder, as they are non-uniform and similar in intensity to the lemons (which we do not want). First we subtract the oranges, since those do not need to disturb the image no more. Then we could go on with two approaches. One could be to run a circle fitting algorithm on the objects. With this approach the lemons would likely to be registered more deviating from a circle than the apples, and thus we would be one step closer to eliminating them. This would of course mean that we first should try to exclude the bowl in which the fruit lie, as a circle would be fit to the inner and outer rim as well.Another approach is to work with the k-Nearest-Neighbour (or k-Means)  clustering methods, looking at an automated segmentation by classifying based on the difference from the nearest 3 or maybe 5 pixels. This could however identify the two oranges in the top-right part of the picture as one entity, which is alright in this case, as we are not interested in finding out how many fruits of a certain class exist, but rather which.Issues may include the light-reflectance bias given from the fruits, as they do not represent the true intensity of the given object. Here we could do a bias correction by thin-plate-splines or the like, to correct for such issues. It does not however seem necessary to attain the sought results.","It is when you do not represent the true nature of the data underlying, but instead start to model the noise.","There are different methods applicable to different models. The most general way is to simply avoid having too many parameters, to cross-validate the model fitting multiple times, and understanding how to do proper diagnostics of the model used.","SELECT COUNT (DISTINCT followee) FROM following;I am not completely sure I understand this question.",Hadoop/Hive is an excellent framework for fast SQL queries.,"You cannot really. I would do multiple random samples by constraining on the number of rows I would extract, do a sanity check of those outputs. If no errors were found, I would proceed to extract the amount I wanted initially.","A neural network inspired by the visual receptive field in biological creatures. The network takes an input and dissects it into many small pieces, this way the nodes only get exposed to small specific parts of the input. ConvNets are computationally quite efficient, and highly useful in image based recognition such as handwriting recognition.","Usually they occur in computer vision and natural language processing cases. But indeed like many other ML methods, it may be used in any problem you wish to use it for. The question is rather how well the problem underlying fits with the model.","There are unsupervised ways of training the ConvNets, but they are very computer intensive, and as such difficult to work with. More common it is to use already initialised convolution nets found online, that already are more or less applicable for the problem you seek to solve.Pretraining is a way to work out the weights of the network, however it is very timeconsuming and one often risks to get stuck in a local minima, which then obstructs the lower layers from learning features. One can avoid this by doing an unsupervised pretraining procedure, however, as written before, this may be quite time consuming.",I would like to learn and improve my skills in data analysis. Furthermore I hope to dig into interesting data sets and spend time with likeminded peers.,5
2016/02/08 2:15:20 pm GMT,Alina Leidinger,Mathematics,,Matlab. It is commonly used in machine learning and I'm using it for my machine learning research project.,"NumPy, SciPy; I'm proficient in using Matlab and R",No,"data = [[3 8 14 13 1];[10 4 17 3 10]; [21 3 1 3 2]; [2 1 20 12 14]; [4 2 4 4 6]];data = cumsum(data,2);figure();bar(data, 'stacked')set(gca,'XTickLabel',{'M', 'T', 'W', 'Th', 'F'})title('user activity per service');ylabel('DAU [10e3]');xlabel('Weekday');legend('A', 'B', 'C', 'D', 'E');","Daily active users, shows the popularity of an application for example","Clarity, how easy it is to communicate insights to an audience. http://www.ted.com/talks/david_mccandless_the_beauty_of_data_visualization?language=enHe presents data in a simple and beautiful way.",May-36,"23. No not the same for real data, some days are more likely for birthdays","Use buffon's needle experiment, dropping matches onto a striped floor","categorise according to coloration, shape/outline; Issues: lemons will likely be mistaken for apples",Choosing a model with too many free parameters.,"various techniques eg rotation estimation, regularisation etc",I don't know much SQL,no,Number of followers cannot exceed number of persons and is unlikely to get close to that total number as well. Make comparison to number of 1st degree followers to see if answer is realistic,I have not studied convolutional networks yet. ,I have not studied convolutional networks yet. ,I have not studied convolutional networks yet. ,"Improve my skills at data science and gain some new programming abilities, learn more SQL. Move towards career as Data Scientist. Gain experience for PhD that I am planning to do.",3
2016/02/08 3:04:28 pm GMT,Alex Capras,"Other (please, specify)",Mechanical Engineering,"Java - First language I learnt. Simplest to use. More confident in Java than any other language. Can write programs quicker in Java than any other language. (It clearly has drawbacks when compared to C++, but I'm far less confident in C++ and don't have that much experience).Python is also fun to play with.Matlab useful for graph plotting etc. Easy for quick scripts.",n/a,n/a,"% LINK: https://drive.google.com/file/d/0ByEdz617lM6SQmFJMVQ4RkVRdUk/view?usp=sharing% MATLAB SCRIPT% @author ALEXANDROS CAPRAS% 08/02/2016% DATA SCIENCE SOCIETY ONLINE ASSESSMENT% WEEKLY DATA BAR CHART%clean upclear all; clc;% read in file and store in 'data' matrixdata = csvread('weekly_data.csv',1,1);% create empty table of same size to put ordered dataorderedData = zeros(5,5);% order data - flip columnsi = 5;for j = 1:5    orderedData(:,j) = data(:,i);    i = i - 1;endfigure %create a new figure% make a stacked bar chart using the ordered data% make each bar 0.5 wide (when width=1, adjacent bars are touching)b = bar(orderedData, 0.5,'stacked');% set title, x-y axis labels and legendtitle('user activity per service')xlabel('Week day');ylabel('DAU[10e3]');legend('A','B','C','D','E');% set the colour of each bar segment using RGB valuesb(1).FaceColor = [0.25, 0.30, 0.77];b(2).FaceColor = [0.55, 0.70, 1.00];b(3).FaceColor = [0.85, 0.85, 0.85];b(4).FaceColor = [1.00, 0.60, 0.45];b(5).FaceColor = [0.70, 0.00, 0.15];ax = gca;  % configure grid linesax.GridLineStyle = ':'; % make grid line dottedax.Layer = 'top' % put gridlines on top of barsax.LineWidth = 0.9; % nice line width value (0-1)ax.GridAlpha = 0.8; % play with transparency of grid linesax.YTick = 0:10:50; % set y-ticks [0 10 20 30 40 50]ax.XTick = 1:5; % set x-ticks [1 2 3 4 5]ax.XTickLabel = {'M' 'T' 'W' 'Th' 'F'} % replace x-axis numbers with days of weekgrid on % show grid lines% END OF SCRIPT",,,"5/36 = 0.1389% @author ALEXANDROS CAPRAS% 08/02/2016% DATA SCIENCE SOCIETY ONLINE ASSESSMENT% DICE THROWclear all; clc; %clean upcount = 0; % counter for total number of combinations where sum = 8total = 0; % counter for total number of combinations for 2 dicefor i = 1:6    for j =1:6        total=total+1; % always increment total        if(i+j==8)            count=count+1; % only increment count if sum is 8        end    endend% assume both dice are unbiased so probability of each event is equaltotalProbability = count/total% END OF SCRIPT","% @author ALEXANDROS CAPRAS% 08/02/2016% DATA SCIENCE SOCIETY ONLINE ASSESSMENT% BIRTHDAY PROBLEMclear all; clc; % clean up% Finding the number of people you need in one room% such that the probability of at least 2 people having% the same birthday is equal to 'desiredProbability'%% The probability of at least two people having the same% birthday is equal to 1 - (probability that nobody has samne bday)% % probability that nobody has same bday is:% p = (365/365)*(364/365)*(363/365)*...((365-N+1)/365)desiredProbability = 0.5;probability = 1; % starting probability is 100%possibilities = 365; %first person can have any birthdayN=0; %number of people, increment each loop% continue looping until desired probability is reachedwhile(1-probability<desiredProbability)    % use formula outlined above    probability = probability * (possibilities/365);    possibilities = possibilities - 1;    N = N + 1; %loop counterenddisp(N) %show result--------------------------------------------------------------------for a desired probability of 0.5, N=23- Assumption: all birthdays have equal probability of occuring. Not true, in reality some birthdays occur much more often than others.","% @author ALEXANDROS CAPRAS% 08/02/2016% DATA SCIENCE SOCIETY ONLINE ASSESSMENT% ESTIMATING PIclear all; clc; % clean up% The area of a circle of radius 1 is PI% The area of a square that contains a circle with radius 1 is 2*2 = 4% By generating N random points in a square, we can estimate the area of % the circle by finding the number of points that lie inside the circle (Ni)% A_circle = A_square * (Ni/N)% For a square of sides 2, the largest circle contained within this square% has radius 1. Therefore the circle will have an area of PI*(1^2) = PI% Hence we can estimate the value of PIA_square = 4; % Area of square; 2x2=4N=100000000; % N random points to be generated inside square (larger N -> more accurate estimation of PI)Ni=0; % Ni: number of points that land inside circlefor i=1:N    randX = rand; % built in matlab function for    randY = rand; % random number between 1 and 0    if(((randX^2)+(randY^2))<1)        Ni=Ni+1; %if the point lies within the cirlce, increment Ni    endend% estimate PIpiVal=A_square*(Ni/N)% END OF SCRIPT","- Scan image- look at pixels- if groups of red pixels found, greater than specified area --> apple- if groups of orange pixels found, greater than specified area --> orange- problems:--> not all apples are 100% red (must have a spectrum of allowable RGB values)--> not all oranges are 100% orange.. same problem--> background is brown, similar color, but it isn't an apple or orange","Didn't know, googled it, wikipedia gave this:""In statistics and machine learning, overfitting occurs when a statistical model describes random error or noise instead of the underlying relationship. Overfitting generally occurs when a model is excessively complex, such as having too many parameters relative to the number of observations.""","Only make your model as complicated as you can afford to. i.e. include assumptions that you know to be true, but not assumptions that you don't really know that much about.I'm just guessing here, don't really know anything about overfitting.",,,,,,,"Learn more about data science - prepare me for career in Technology/BigData/Software Developing, or something in that direction. Learn more programming languages that are more applicable to these types of problems (e.g. R, Python, SQL etc)Have fun, meet people, etc.",3
2016/02/10 3:11:08 pm GMT,Daniel Bunting,Physics,,"Python, great for rapidly getting ideas up and running but still has got options for optimising for performance later.","sklearn, pandas, scipy, numpy, ROOT",Have some stuff on https://github.com/dnlbunting but not much.,"import numpy as npimport pandas as pdimport matplotlib.pyplot as pltcolors = ['#3E509E', '#91ACDA', '#DCDBDC', '#F3977A', '#B41828']types = ['A', 'B', 'C', 'D', 'E']data = pd.read_csv(""weekly_data.csv"")cum_data = np.cumsum(data[types], axis=1) - data[types]for i, day in enumerate(data['Day']):    el = plt.bar(left=[i+0.5]*5, width=0.5,            height=data.ix[i][1:],             bottom=cum_data.ix[i],             color=colors)    if i==0:        plt.legend(el, types)    plt.title(""User activity per service"")plt.ylabel(""DAU[10e3]"")plt.xlabel(""Week day"")plt.xlim(0,5.5)    plt.xticks(np.arange(5)+0.75, data['Day'])plt.grid()# Pretty sure your figure is incorrect- e.g. the largest bar in W is A with 21, but you have it coloured red which is E",Daily active users? Representative of how many people are engaging with your website ,"Clarity whilst conveying lots of information.Something like this http://static-content.springer.com/image/art%3A10.1186%2Fgb-2013-14-4-111/MediaObjects/13059_2013_3026_Fig1_HTML.jpg is really cool.",May-36,"23. This assumes there are 365 equally likely days to be born on, so ignores leap years, twins, day to day and seasonal variations in birthday...more people are born 9months after valentines day ;)  ","Monte carlo method: simulate points uniformly distributed over a square with sides 2a, find the fraction that are inside a circle inscribed in the square, which will tend to pi*a^2/4a^2 so you can find pi from that.","The simplest way would be to identify the fruit as circles and the colour and the radius as features to train a classifier.Likely issues would be low contrast with background and non-spherical fruit would make it hard to extract the features.",Training a model to a particular dataset so much that it generalises poorly to unseen data.,"Cross validation. Basically, splitting the data into train and test sets and using the train set for fitting and the test set only for measuring performance. This is obviously wasteful when there is limited data so using methods like leave one out CV or out of the bag scoring can help.",Never used SQL...,,,"Convolutional layers in neural networks detect spatially localised features e.g. faces by connecting small groups of pixels, repeated over the entire visual field. ","Mostly image processing, for example famously recognising cats in youtube clips",Gradient descent/back prop. Pertaining can help with vanishing gradients in deep networks ,"Interesting challenges and problems to work on, lots of learning! ",3
2016/02/10 3:17:02 pm GMT,Lorraine Choi,"Other (please, specify)",Engineering,"A tie for R and Python - R for its strong data visualisation capabilities and Python for its easy-to-understand syntax. R is also very good at data analysis tasks with all the statistical packages, but given Python is a general-purpose language, it is more convenient when integration with web apps or production database is needed, and generally computes quicker as well.","data manipulation: dplyr (R), plyr (R), data.table (R), pandas (Py)data analysis:  NumPy  (Py), SciPy (Py)data visualisation: lattice (R), ggplot2 (R), matplotlib (Py)machine learning: caret (R), scikit-learn (Py)some other common open source libraries such as OpenCV and Torch",https://github.com/lorrainechoi,"library(ggplot2)library(reshape2)data <- read.csv(""weekly_data.csv"", stringsAsFactors = FALSE)data$Day <- factor(data$Day, c(""M"",""T"",""W"",""Th"",""F""))data <- melt(data, id.vars = ""Day"", measure.vars=c(""A"",""B"",""C"",""D"",""E""), value.name = ""DAU"")ggplot(data, aes(x = Day, y = DAU, fill = variable)) + geom_bar(stat = ""identity"", position = ""stack"", colour=""black"") + ggtitle(""user activity per service"") + xlab(""Week day"") + ylab(""DAU[10e3]"") + theme(legend.position = c(1,1), legend.justification=c(1,1), legend.title=element_blank())",Daily Active Users - it measures how successful a product is to keep users engaging with it.,"To present information in a simple and efficient way, where stakeholders could easily understand complex concepts or identify new patterns/trends from it.US Thanksgiving on Google Flights is an amazing dynamic visualisation of the flight traffic in America on Thanksgiving day. It shows clearly how a few cities are the main departure/landing sites (eg Florida, California, New York) across different times and viewers could easily understand the complex data.http://goo.gl/xz8djR",5/36 = 14%,"23 people, but it only applies when people are being chosen randomly. In reality, when you invite someone to your party, your won't pick your guest randomly since there are many factors that would influence who to invite such as theme of party, relationship among guests etc.","Could be done by using a circle with radius R bounded within a square (2R * 2R). Since ratio of area of circle to the area of square is pi/4, probability of a randomly selected point to fall inside the circle is pi/4. The ratio of the no. of points that is within the circle (X) and no. of total points (Y)  taken should be equivalent to the ratio of the area of circle to total area (area of square), i.e. (X/Y) = (pi/4). So pi = 4*(X/Y)","From our personal knowledge we know the appearance of apples and oranges - features such as shape, colour, size and smooth/rough surfaces could be used to categorise the fruits. Such as if it is round, orange in colour and has a rough surface, then it's likely to be an orange. If it is red or green, and has a round shape with a dip on top, then it is likely to be an apple. ","It describes noise as trends, or essentially providing misleading interpretation/over-generalised trends using error data points. Overfitting could occur when over-complex model ,such as too many classes or parameters, is used, or learning was too long, or training data set size is small.",Could use techniques such as cross-validation or early stopping to halt learning process when sufficient data is being fed to system to train it.,"SELECT t1.Followee, count(distinct t2.Follower)FROM t_new t1 JOIN t_new t2 ON t1.Follower=t2.FolloweeGROUP BY t1.Followee;","No but R could do similar joins easily using ""apply""","Pull a smaller data set to test first, like limit to 3 followees and have a quick check to see if numbers are right. Or if similar analysis done previously is available, it could be used as a reference.","ConvNet is evolved from AI and deep learning, and uses neuron simulations as a form of machine learning to process data, mainly used in the visual field. A network consists of a few convolutional layers and layers are connected to form a 3D architecture. ",ConvNets is widely used in computer-aided analysis of medical images when combined with image processing.,,"To gain more experience in data science side - currently it's very difficult to have hands-on experiences in the industry, and ICDSS seems to give a good opportunity for me to get in touch with the industry and learn more about data science.",3
2016/02/12 8:28:06 am GMT,Jeremy Desir,"Other (please, specify)",General Engineering (major in Data Science),"R as it contains several statistical libraries and I have been used to code with it in my previous engineering school. Moreover, I find it very user friendly !","Strictly speaking there are packages : ""FactoMineR"" (the first one I've been using when I discovered R and had to code a PCA for then making a comparison with the existing one, but it also contains clustering function like HCPC), ""earth"" (thanks to which I discovered the impressive properties of MARS regression), and ""DiceKriging"" that has been recently created by my previous teachers at Mines Saint Etienne. It is based on brand new approach of computer experiments and meta modeling. ",,"weekly_data <- read.csv(""~/Downloads/weekly_data.csv"")WD<-as.matrix(weekly_data[,2:6])for (i in 1:5) WD[i,]<-rev(WD[i,])rownames(WD)<-as.vector(weekly_data[,1])barplot(t(WD),width=0.5, space= 0.3,legend.text = c(""A"",""B"",""C"",""D"",""E""),        col=c(""darkblue"",""blue"",""grey"",""orange"",""red""),        xlim=c(0,5),ylim=c(0,50),        xlab=""Week day"",ylab=""DAU [10e3]"",        main=""user activity per service"")",,"The most important thing in data visualisation is the fact that meaningful but complex ideas can be explained and shared easily tanks to it. As Dr.Rosling shows in the following TedTalk (https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen), tremendous change on the world can be captured with the good software, displaying te good visualisation of data across space and time. The part of its speech where he deals with the distribution in the world of the income is fascinating. Once again, thanks to its well designed software, he succeeded to highlight how the ""hump"" between the richest and the poorest people in the world has faded away from 1970 to nowadays, and how each geographic class of the world population has evolved. By ""letting the world move forward"" through a sort of movie describing time evolution of people income according to their origins, a meaningful pattern can be quickly extracted which is really interesting and allows one to ""see information"".","There are 5 favorable combinaisons (D1={2,3..6} and D2={6,5..2}) out of 6*6=36 possible combinaisons. Thus the answer is 5/36.  ","Let's first think about the probability that two persons taken randomly DO NOT have their birthdays on the same day. There is 365 possibilities out of 365 for the first one and 364 out of 365 for the second. Then if we add a third person, there is just 363 possibilities out 365. So for n people, there are 365*364*...*(365-n+1) possible sequences. To come back to the question, we need the smaller n such that :365*364*...*(365-n+1) / 365^n  < 1/2 which is resolved with n=23If we have real data, the probability of two real numbers taken randomly being the same is always 0. Then we would need an infinity of ""people"" to be sure that at least 2 of them have the same birthday.","One can do so with a Monte Carlo simulation. Let's consider a circle of radius 1, its area is theoretically pi. If we take x and y two uniformly distributed random variables belonging to [-1,1], we can access any point within the cercle if x^2+y^2  <1 . Based on this idea and the central limit theorem,  if we generate N (a huge number) times x and y independently, and count the average number of times the value x^2+y^2 is lower than 1, the result should converge to the circle area, which is pi. ","With a binary classifier like a logistic one for example. The typical issues would be to chose wisely the ""decision boundary"" classifying a fruit as an orange rather than an apple (if they look very similar, with the exact same shape and color for exemple). Another problem is overfitting. ","Overfitting occurs when our predicting model is ""too close"" of the training data. Technically speaking, it's about preferring  a low biais and large variance to a large biais and low variance.","Thanks to penalization techniques such as shrinkage methods, one can improve the overall prediction accuracy by sacrificing a little bit of biais to reduce variance. In the linear regression context, we impose a penalty on regressors coefficients size. The most renowned shrinkage methods are lasso and ridge.",,Maybe Access ? or Apache Hadoop ?,,,,,"I would like to get a wider culture on both current challenges raised by data science / big data phenomenon on business for the next decade and also some insight of trendy machine learning algorithms, clustering techniques or whatever technical aspect which is currently used by data scientists. To do so, presentations and workshops from professionals already involved in this sector (working for either tech firms but also industry, finance, marketing..) would be amazing !",3
2016/02/12 8:05:08 pm GMT,Hermann Blum,Computer Science,,"Python- broad range of open-source frameworks and tools- it is rather easy to write clear and maintainable code- as numpy is optimized in C, performance still not that bad","numpy/scipy as the basis of everything, on top of that:- pandas -> makes data handling even better- scikit-learn -> machine-learning implementations- sqlalchemy -> easy database usage","https://github.com/hermannsblumHowever, most of my work has been team-work hosted at other accounts, including the example I send you in the e-mail:https://github.com/NotSpecial/mapomatv2 -> Map visualization of business location densitieshttps://github.com/amiv-eth/amivapi -> API for my local students association in Zurich","The following is  a python script for py3.5, tested in jupyter notebook (therefore the marplot inline might not work properly in your cli)You will find a difference in the bar heights as there is a huge mistake in your barplot above (I hope you intended this)?? -> look at Wednesday: A should be the biggest value, but it is plotted in the color of E. If you really want me to manipulate data that is possible but a little bit senseless, therefore I just plotted the data as you provided it.result: https://www.dropbox.com/s/8ppj59bgxr34z5i/ICDSS.png?dl=0import numpy as npimport matplotlib.pyplot as plt%matplotlib inlineimport pandas as pd# now load the data into pandas DataFramefilename = 'weekly_data.csv'df = pd.DataFrame.from_csv(filename)# plot as a stacked bar plot using the colors provided (don't know the colormap therefore I just picked the colors out of the provided image)df.plot(kind='bar', stacked=True, rot=1, color=['#4737C4', '#8DABFE', '#DEDDDD', '#FD8F73', '#C4000D'])plt.xlabel('Week day')plt.ylabel('DAU [10e3]')plt.title('user activity per service')plt.grid()plt.show()","DAU = either Daily Active Users or DÌ_mmster Anzunehmender User (German abbreviation ;), stands for most probably stupid user)I assume however that you mean the first.Daily Active Users is a measure for the popularity of your webpage/service. It allows analysis of the following questions: How many users are there on average per day?Does the user behaviour vary throughout the week?Especially the first question is important if the service relies on advertisement.","Most important: Show connections/correlations in an easily understandable and transparent way, that means the audience is able to understand connections by themselves and is at the same time able to easily proof your sources and set the graphic into a context.one of my favorites: https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen#t-955143 -> data is easily understandable and after a few minutes you have a really good overview about what is happening in the world (of course, the animations and flash usage and everything is outdated x10 ;) )","possible combinations with sum 8: 2+6, 3+5, 4+4, 5+3, 6+2total number of combinations: 6*6 = 36-> probability for sum being 8: 5/36 ","assuming that the date of birth is equally distributed among all 365 days in a year (and omitting 29.2. because it is only 1/4 as likely as every other day and I am an Engineer)for n persons:possible combinations that all people have different birthdays: 365 * (365-1) * ... * (365 - n + 1) = 365! / (365-n)!all possible combinations: 365^nprobability that at least 2 persons out of n have the same birthday: p = 1- 365! / (365^n * (365-n)!)-> p >= 0.5 for n >= 23the second is a tricky question. For n >= 23 the possibility is higher for persons to have the same birthday than for everybody to have a different birthday. Therefore, the expected value of a discrete random variable mapping the 2 cases into 0 (all have different birthdays) and 1 (at least 2 persons share a birthday) is p >= 0.5, which makes 1 a better guess than 0. However, the variance will be quite high for low n (for p=0.5/n=23 the states 1 and 0 are equally likely -> like flipping a coin, so don't bet too much money) and the confidence that >= 2 persons share a birthday will increase with n.Additionally, I assumed an equal distribution above and omitted Feb. 29. I think I remember a study that people are more likely to make children in summer because conditions are better (no winter depression, more jobs, more time due to holidays, ...). Therefore, the assumptions are not quite right. If there are high-density intervals of birthdays in the year, n from above would be even lower for the same p. ","I am sure there are much better ways to estimate pi, but as the standard gaussian distribution follows the PDF= 1/(\sqrt{2\pi} \exp{-x^2/2}, one can take a really big sample of this distribution and following the law of large numbers the precision of the calculated pi out of the distribution should increase with the number of data.After a little bit of research I found another method with a uniform distribution of points in a square (-1, -1) to (1,1) and you measure the points which are within distance 1 from the origin. This method is better because the fraction between numbers outside and inside the circle is directly linked to pi.However, in principle both are quite the same: I search for a distribution that is somehow dependent on pi, measure a process oh which I know that it follows this distribution and use the sample to get an estimate for pi.","I would basically look at the following features: colour, size, shape. The color should basically be enough to separate oranges from the rest and then the size would tell whether it is an apple or a lemon.Issues are of course - general detection of single object bounds in the bowl because of the wood texture (which will lead to detected objects where actually no abjects are)-apples have different colors and in general not really one plain color and are of the same size as oranges. Therefore, an error in the single feature ""colour"" can be enough to lead to a false classification. One could search for the stem (is this the correct word?) or other patterns in a fourier transform of the object image, because apples are less plain in color they should have higher frequencies and introduce this as another feature.","If your classifier develops rules that are too specifically linked to the training set that they are not general anymore, i.e. the error on the training set decreases but the error on other data (e.g. validation sets) increases, because the really specific rules lead to correct classification in the training set but random clasification for random other data.","use a part of the available data not for training but for validation, i.e. for every tested set of parameters you derive the error on the validation set. As this data was not available to the training algorithm it should in general indicate overfitting. Important is that validation and training sets is a random choice from the data and do not overlap.If the number of data is limited there are methods like cross-validation which I am sure you know about ;)","(tested with sqlite)select followee, count(follower) from (select distinct a.followee, b.follower from following a, following b where a.follower=b.followee and not exists (select * from following where followee=a.followee and follower=b.follower)) group by followee;","I would guess that it makes it slower but I prefer querying any kind of database with a wrapper like sqlalchemy. This way, you can store and access intermediate results without having to create tables and split up the code in smaller steps which makes debugging and bug fixing a lot easier. ","The easy answer to this: You can never be sure that your code is correct (maybe with functional programming and/or an infinite number of unit tests...).However, I am not quite sure what this question is about.","ConvNets are a specific kind of feed-forward ANNs. They can be split into the convolutional part and the fully connected part. In the convolutional part, the size of the input (which is considerably high for an image, e.g. 256x256) is reduced in multiple steps to a lower size input for the fully connected ANN. The idea is to train the convolutional part of the network in a way that the information in the input of the fully conncted ANN (do they call it here also latent space like for PCA?) is nearly the same as in the original image with respect to the given network tast, e.g. face recognition/object classification/... A convolution works as following: You take a sub-square of the image, e.g. 3x3 and map it into one pixel. Then you shift the sub-square by 1 pixel and do the same again.This way, each pixel from the reduced size image has information from a lot of different pixels in the original image and the information of one pixel in the original image is part of a lot of pixels in the reduced size image. That is why it's called convolution.","I only know about their usage in image analysis. Therefore, possible applications are listed above: face recognition/object classification/... The task to detect oranges and apples is a good field of application for ConvNets","Don't know about that, never worked with ConvNets.",#NAME?,4
2016/02/13 9:41:12 am GMT,Edoardo,Mathematics,,"Matlab, the syntax is intuitive and it's very powerful.","C++, Matlab, R",No I don't.,,,,01-Jun,you need 23 people,you scatter randomly 10000 points on the unit square. Then you take the number of points on the circumference of the unit circle and divide them by the number of points on the diagonal of the circle,"Using neural networks, I would create three categories: 1 for apples, 2 for oranges and 3 for other fruits. I expect that given so few examples it would be hard to train a well functioning network.",Overfitting means that a model describes the training set extremely precisely and has trouble generalising to other sets.,"In machine learning you can prevent overfitting by using regularisation, averaging the outputs of several networks, using smaller training sets, randomising the training and validation sets.",,,,,,,,3
2016/02/13 11:21:34 am GMT,Calum Beddow,Mathematics,,"Python. It is the language I first learnt about computer science in and the stuff i'm going to go on to do will probably require quick development and flexibility that python offers, whilst still giving reasonable performance.",I have had a bit of exposure to NumPy/SciPy. Mainly used packages within R.,I have one but it's empty. https://github.com/beddow,"#Using R, colours, dashed gridlines and legend positioning are wrong require(reshape2)require(ggplot2)data <- read.csv(file=""Downloads/weekly_data.csv"",head=TRUE,sep="","")data.M <- melt(data)data.M <- data.M[order(nrow(data.M):1),]data.M$Day <- factor(data.M$Day , levels=unique(as.character(data$Day)))c <- ggplot(data.M,aes(x = Day, y = value, fill = variable)) +      geom_bar(stat = ""identity"",aes(width=0.5)) +      theme_bw() +      xlab(""Week day"") + ylab(""DAU [10e3]"") +       theme(legend.title=element_blank()) +      ggtitle(""user activity per service"") +      scale_colour_manual(values=c(""blue"",""green"",""orange"",""red"",""white"") )  +      guides(fill=guide_legend(title="""")) +      scale_y_continuous(limits=c(0, 50), expand = c(0, 0))  c","Daily Active Users - shows how many unique users interact with your product, advertisers probably make heavy use of it as they can define an ""interaction"" across all websites/applications and it provides a consistent metric that it is relatively easier to measure.","You're doing it to gain insight that can help guide actions, it needs to get the key points across to everyone it is intended for in an unbiased fashion. This: http://cardsagainsthumanity.com/holidaystats/ Made a good story from a few data points.",5/36 or ~0.14,"23, i would expect it if i observed enough parties","Monte carlo method with a circle of radius 1 inscribed within a square (of width 2). Generate u1,u2 ~U[0,1] then use x1=2u1-1 and x2 = 2u2 -1, continue to generate xi values and use indicator function on the circle to assign the value 1 if they fall within the circle, 0 otherwise. Sum up these indicated values and divide by n and this should approximate pi/4 for n large, so multiply the estimated value by 4 to get an estimate for pi.","Make a tree and look at colour and size. the orange colour stands out quite clearly, so first classify those. The apples and lemons have a similar colour so call it an apple if it is larger than some value you will have to work out. The issues are that lighting might distort colour and apples may be particularly small or lemons particularly large.",When a model is too complex and fits to random error in data rather than the underlying data.,Reduce model complexity. Cross validation. Regularization terms.,"This doesn't work :) It doesn't exclude if someone is both 1st and 2nd degree follower, but i've never done SQL.SELECT f1.Followee, count(distinct f2.Follower)FROM Following f1 INNER JOIN Following f2 ON f1.Follower=f2.Followee AND f1.Follower<>f2.FollowerGROUP BY f1.Followee",Not any query languages,Move to something more scalable like NoSQL?,"I've never used them, apparently it's like an MLP but not everywhere fully connected so can scale better with 2D input.",e.g. image recognition,Back propogation,Build an understanding of real-world data science. Practice some fundamental skills and learn with others and from their skills. ultimately go on to a job where data science is a key part of the role,4
2016/02/13 11:37:31 am GMT,Scott Ward,Statistics,,"R, good for statistical analysis",0,No,"#Using Rlibrary(gdata)weekly_data <- read.csv(""weekly_data.csv"")par(mar=par()$mar+c(0,0,0,4))barplot(t(as.matrix(weekly_data[,seq(2,ncol(weekly_data))])), main=""user activity per service"", ylab=""DUA[10e3]"",         col=heat.colors(5), xlab=""Week Day"", space=0.1, cex.axis=0.8, las=1,        names.arg=c(""M"",""T"",""W"",""T"",""F""), cex=0.8) legend(5, 45,names(weekly_data)[seq(2,6)], cex=0.8, fill=heat.colors(5));",DAU = Daily active users. It is a measure of the success of a product sold through the internet.,Proper scales to allow for ease of comparisons between data. ,May-36,254. No due to possible relationships between individuals and possible seasonal trends of birthdays.,Monte Carlo,"By using skin texture, size, colour and weight. With colour, the apples exhibit two different colours. Issues also arise with attempting to classify objects that do not fall within apples and oranges.","When a model attempts to explain random noise in the data, in addition to the underlying process.",By using statistical tests (e.g. Wald tests) to remove parameters from the model.,,,,A type of feed-forward artificial neural network.,Image recognition,,More programming experience and the opportunity to apply statistical methods to real data,1
2016/02/13 11:59:36 am GMT,Tommaso,Mathematics,,R,"python library theano and cariss for machine learning, Topic modelling with gensim, Keyword extraction using the python library nltk) ",https://tgp13@bitbucket.org/ImperialHPSC/m3c2015.git,"tabar <- read.csv(file=""H:\\R\\R\\weekly_data.csv"",sep="","",header=TRUE,stringsAsFactors=FALSE,row.names=NULL)head(tabar)tabar1<-as.matrix(tabar[,-1])barplot(t(tabar1), main=""user activity per service"", xlab=""Week day"",ylab=""DAU [10e3]"",         col=c(""blue"",""cornflowerblue"",""azure3"",""coral"",""brown4""), space=0.1, cex.axis=0.8, las=1,        names.arg=c(""M"",""T"",""W"",""T"",""F""), cex=0.8) legend(5, 45, names(tabar[,-1]),cex=0.8, fill=c(""blue"",""cornflowerblue"",""azure3"",""coral"",""brown4""))","Daily Active Users is a metric that measures the popularity of a social media for example at any given time, plus it is an easy to get metric unlike revenue per user etc. ",Ability to highlight features of the data which can only be fully appreciated with a data visualization. A picture is a 1000 words becomes even more true in a good data visualization. Interactive visualizations draw the viewer in pro actively like this: https://datafloq.com/read/how-brands-can-use-social-media-to-enable-geo-targ/1839?utm_source=Datafloq%20newsletter&utm_campaign=976921f873-Datafloq_newsletter_2_8_2016&utm_medium=email&utm_term=0_655692fdfd-976921f873-77600589,May-36,254,Monte Carlo,Feed Forward Artificial Neural Networks,When a statistical model describes random error instead of the underlying relationship,"Not having too many explanatory parameters in your model or in neural networks to use the ""dropout technique"" to reduce overfitting by  dropping out units hidden and visible to prevent complex co-adaptations on training data",,,,A Type of Feed Forward Artificial Neural Network used in image recognition. This has the advantages that it can tolerate translation of the input image by design and compared to other image classification algorithm requires little pre processing as the network is responsible for learning the filters. ,"Image/video recognition, recommendation systems and natural language processing",Use a test training data set with the Stochastic Gradient Descent and/or back propagation algorithm,Insight into Machine Learning and cutting edge data science techniques,3
2016/02/13 12:42:19 pm GMT,Tinatini,"Other (please, specify)",Operations research,"R,python.  ","networkx, igraph,Lpsolve,NumPy,NLTK,matplotlib",no,"table<-read.csv(""weekly_data.csv"")mat<-matrix(c(table$A,table$B,table$C,table$D,table$E), ncol=5)colnames(mat)<-c(""M"",""T"",""W"",""TH"",""F"")rownames(mat)<-c(""A"",""B"",""C"",""D"",""E"")as<-as.table(t(mat))barplot(as, col=c(""darkblue"",""lightblue"",""grey"",""orange"",""red""), legend = rownames(mat),+ xlab=""week day"", ylab=""DAU[10e3]"", main=""user activity per service"", border = TRUE,lwd = 2 )",no,"http://www.oecdbetterlifeindex.org   easy to understand complex data, nice color choice.",01-Sep,24,MonteCarloMethod,orange has more dark color and in shape it is smaller than apples. collor of the apples is not smae.,overfitting in Machine learining is when your training data performs well on the training data but does not perform well on the testing data,cross validation method,,no,,type of deep learning in ML,,,,4
2016/02/14 12:57:41 am GMT,Ciro Monti,Statistics,,R - very easy to do stats/ML on it ,"R - dplyr, plyr, data.table, stringr, zoo, ggvis, lattice, ggplot2, caret Python - NumPy,SciPy, pandas, matplotlib, scikit-learnfaves - ggplot2 & pandas",https://bitbucket.org/cm3412/,"library(ggplot2)library(reshape2)data = read.csv('/Users/ciromonti/Desktop/weekly_data.csv', header = T)melted <- melt(data, id.vars=c(""Day""))qplot(x=Day, y=value, fill=variable,      data=melted, geom=""bar"", stat=""identity"")+      ggtitle('Made a prettier version ;)')+      labs(x = 'Week day', y ='DAU[10e3]')","Daily active users. Yes, I can imagine it is a popular unit (although somewhat flawed as it may overcount)",Data visualizations needs to communicate as much as possible in the SIMPLEST way possible. Its also important to account to uncertainty/noise/errors.,May-36," 23 assuming that each day of the year (except February 29) is equally probable for a birthday. In real life, this assumption might be a bit strong as birthdays are seasonal - so I wouldn't expect the same number with real data","Monte Carlo -Pick a random point (x, y), where both -1<x,y<1. the probability of that this random point lies inside the unit circle is given as the proportion between the area of the unit circle and the square : p(x^2+y^2<1) = area of circle/area of square = pi/4.Pick N such random points and note that n lie in unit circle. Then pi  = 4n/N",Categorise by colour and shape. Issues - computer may struggle to detect orange/apple colour/yellow/table colour. Apples are yellow and reddish - code may split them into two separate fruit if not well built,When you fit both the signal and the noise (instead of just the signal),"using train/test sets and comparing type 1,2 errors. In general, using simple, tractable models where possible or use models that account for noise such as Gaussian Processes",Dont know SQL :( ,Dont know SQL :(,Dont know SQL :(,"They're a type of neural net inspired by biological processes. Similar to MLPs but scale better. Unfortunately, I don't know much more but they seem interesting and I'm keen to learn","image/video recognition, recommender systems, NLP and any sort of complex non-linear regression I suppose",Not sure as I don't know much about conv nets :(. Pretraining might be for hyperparameters (if conv nets have them),Get my hands dirty with some interesting/exciting project + learn some new skills + build something cool by the end of it,2
2016/02/15 12:25:01 am GMT,Mark Whitehouse,Mathematics,Maths with quite a lot of statistics,"Probably R due to it's wide range on in built functions for statistical methods, combined with its enormous base of packages. Thus making it incredibly versatile and easy to use. Although I also enjoy Python but having studied quite a few statistics modules I'm much more experienced with R. I am currently also learning Julia which looks really exciting. ","NumPy, SciPy, ggplot2, cluster, data.table, PyPlot, Distributions, Gadfly",Search for mw3212 on BitBucket - although sorry besides using it for the m3cproject I don't update it very much!,"I thought it looked like a Python plot, but since everyone else would use Python I figured I'd mix it up and make Julia and R versions...Julia code:# Use PyPlot package to create the graphicusing PyPlot# Load the datadata = readcsv(""weekly_data.csv"")# Specify days for x-axis labels and services for legend labelsdays = data[2:6,1]services = data[1, 2:6]# Define the colours for each bar to becolours = [""mediumblue"", ""cornflowerblue"", ""lightgrey"", ""darksalmon"", ""firebrick""]# Create the plot!fig = figure(""pyplot_barplot"", facecolor=""white"") #, figsize=)bar(1:5, data[2:6, 2], color=colours[1], label=services[1], width=0.5, align=""center"")for i = 3:6  bar(1:5, data[2:6, i], bottom=sum(data[2:6, 2:i-1], 2), label=services[i-1],      color=colours[i-1], width=0.5, align=""center"")endtitle(""user activity per service"")xlabel(""Week day"")ylabel(""DAU[10e3]"")xlim([0.5, 5.5])xticks(1:5, days)grid(""on"")legend()# Save plotsavefig(""MyVisGraph.png"")R code:## Use data.table for easier data manipulation, ggplot2 for nice graphicslibrary(data.table)library(ggplot2)## Load the datadata <- as.data.table(read.csv(""weekly_data.csv""))## Melt it for ggplotdata.melted <- melt(data, id.vars=""Day"", variable.name=""service"", value.name=""count"")## Define the colours for each service typemycolours <- c(""A""=""darkblue"", ""B""=""royalblue"", ""C""=""lightgrey"",               ""D""=""darksalmon"", ""E""=""darkred"")## Create the plot!p.bar <- ggplot(data.melted, aes(x=factor(Day, levels=data$Day), y=count, fill=service)) +    geom_bar(stat=""identity"", colour=""black"", alpha=0.75, width=0.5) +    labs(x=""Week day"", y=""DAU[10e3]"", title=""user activity per service"") +    scale_y_continuous(limits=c(0, 50), expand=c(0, 0)) +    scale_fill_manual(values=mycolours) +    theme(panel.background=element_rect(fill=""white""),          panel.border=element_rect(fill=NA, colour=""black""),          panel.grid.major=element_line(colour=""grey40"", linetype=3),          legend.title=element_blank(),          legend.justification=c(1,1), legend.position=c(1,1),          legend.background=element_rect(fill=""white"", colour=""black""),          axis.text=element_text(size=10))## Display the plotprint(p.bar)## Save itggsave(""R-VisGraph.png"", p.bar, width=7, height=6)P.s. Is it just me or does the picture on the form not match the data? Sorry if I've just failed to miss something and that's why my graphs aren't exactly the same!",Daily active users... Probably popular because it shows the variation of users thoughout the week and can be used to track regular/periodic behaviours and predict expected traffic.,"The graph must be detailed correctly and it should be clear what it represents (eg all axes labelled, title, legends as necessary etc). Otherwise it's just a picture but doesn't tell a story, which is the whole point of data visualisation. Don't know what data presentation examples to choose in particular, so generally having a look around https://www.reddit.com/r/dataisbeautiful is a good idea :)",5/36 (=13.8889%),"The birthday paradox! 23 people are needed for at least a 50% chance that at least two people share the same birthday. This number would not be the same for real data, since it is worked out on the assumption that each birthday is equally likely. This is not true due to February 29th, different dates have different birth probability and what if I invited twins to my party...","Monte Carlo methods would be used - for example generate two uniformly distributed random variables on the interval (0,1), x and y say. Then each generation gives a point (x,y), and pi is given by 4*(number points such that x^2+y^2<=1)/(total number of points generated). The number of samples needed to accurately estimate pi can be reduced using methods such as control variates, conditioning or importance sampling.","Could use support vector machines with training data to learn to classify between the different fruits. I would expect that the similar colour shades (green-ish apple and similarly yellow lemon, and darker apple and brown bowl) to make it hard to distinguish between colours, and the streaks in the bowl might cause issues.","Overfitting is where a model fits the data too well, in that is starts to explain noise within the data. This is an issue because if there is natural variation within the data then it would be a better fit if it modelled the overall trends rather than random discrepancies.",Restrict the complexity of the model either by manual selecting the parameters or use an information criterion to penalise models with too higher complexity.,Sorry I don't know any SQL :( (But would be very keen to learn!) But so I would probably construct an equivalent problem using graph theory and solve in R/Python...,No but I know that R has some packages that allow for query-like commands.,Check that the second degree followers aren't following those people directly? Not sure due to my lack of sql knowledge sorry!,"They are a variation of multi-layer perceptrons, and consist of multiple layers of neurons arranged in a 3D volume. However unlike MLPs which have full connectivity, CNNs are only locally connective (exploiting local correlation) so that they do not suffer from the curse of dimensionality. And finally they differ in that CNNs have shared weights, so that all the neurons in a layer detect the same feature.","Image recognition, drug discovery, natural language processing, playing games such as go, and more...","They can be trained on a data set from a similar area since often only a small amount of training data is available. Once the parameters converge then additional training can be done on the small training data set to fine tune to parameters. It is important to avoid overfitting, and otherwise the gradients computed drop in magnitude through each layer so further down the parameters wouldn't be trained so would just be the randomly initialised values.","Knowledge, make some friends, gain some experience working with different types of data and different problems.",4